{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMltdJeCgqpyTJfyMSNNAYw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phonemyat-4116/SentimentAnalysisPrj/blob/roberta-sentiment/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok"
      ],
      "metadata": {
        "id": "BpDUOQzCNVxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a925fcd-6678-4468-b4bf-6c722ab21fb3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.43.2-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.30.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.43.2-py2.py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pyngrok, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 pyngrok-7.2.3 streamlit-1.43.2 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2uGIP7ca9LpQAHmxHGxAn68wjby_6dsq5AnRK8MyVkNkfo65L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a56zaPLkSkTq",
        "outputId": "250968c8-528d-458c-8099-1c5ff8cbc505"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# import streamlit as st\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from transformers import pipeline, RobertaTokenizer\n",
        "# import torch\n",
        "\n",
        "\n",
        "# # Check if CUDA is available and set device accordingly\n",
        "# device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# # Initialize sentiment analysis pipeline with RoBERTa\n",
        "# @st.cache_resource\n",
        "# def load_sentiment_model():\n",
        "#     return pipeline(\n",
        "#         \"sentiment-analysis\",\n",
        "#         model=\"cardiffnlp/twitter-roberta-base-sentiment\",\n",
        "#         return_all_scores=True,  # Return scores for all sentiment classes\n",
        "#         device=device  # Use GPU if available\n",
        "#     )\n",
        "\n",
        "# sentiment_pipeline = load_sentiment_model()\n",
        "\n",
        "# # Initialize the tokenizer\n",
        "# tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "\n",
        "# def analyze_sentiment(text):\n",
        "#     \"\"\"Analyze sentiment using RoBERTa.\"\"\"\n",
        "#     max_length = 512  # RoBERTa's max token length\n",
        "#     # Tokenize the text and check its length\n",
        "#     tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "#     # If the token length exceeds the limit, chunk the text\n",
        "#     if len(tokens['input_ids'][0]) > max_length:\n",
        "#         # Split the text into chunks based on token length\n",
        "#         chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
        "#         results = []\n",
        "#         for chunk in chunks:\n",
        "#             results.append(sentiment_pipeline(chunk)[0])\n",
        "\n",
        "#         # Average the scores across chunks\n",
        "#         neg = sum(r[0]['score'] for r in results) / len(results)\n",
        "#         neu = sum(r[1]['score'] for r in results) / len(results)\n",
        "#         pos = sum(r[2]['score'] for r in results) / len(results)\n",
        "#     else:\n",
        "#         # Process the text if it fits within the token limit\n",
        "#         result = sentiment_pipeline(text)[0]\n",
        "#         neg = result[0]['score']\n",
        "#         neu = result[1]['score']\n",
        "#         pos = result[2]['score']\n",
        "\n",
        "#     compound = pos - neg\n",
        "\n",
        "#     return {\n",
        "#         'pos': pos,\n",
        "#         'neg': neg,\n",
        "#         'neu': neu,\n",
        "#         'compound': compound\n",
        "#     }\n",
        "\n",
        "# def process_uploaded_file(uploaded_file):\n",
        "#     \"\"\"Process uploaded CSV file and analyze sentiment.\"\"\"\n",
        "#     df = pd.read_csv(uploaded_file)\n",
        "\n",
        "#     # Determine which column to use for sentiment analysis\n",
        "#     valid_columns = ['Feedback', 'Review', 'Text']\n",
        "#     selected_column = next((col for col in valid_columns if col in df.columns), None)\n",
        "\n",
        "#     if not selected_column:\n",
        "#         st.error(\"CSV must contain a 'Feedback', 'Review', or 'Text' column.\")\n",
        "#         return None\n",
        "\n",
        "#     # Create a progress bar\n",
        "#     progress_bar = st.progress(0)\n",
        "#     total_rows = len(df)\n",
        "\n",
        "#     # Apply sentiment analysis with progress updates\n",
        "#     results = []\n",
        "#     for i, text in enumerate(df[selected_column]):\n",
        "#         scores = analyze_sentiment(str(text))\n",
        "#         results.append(scores)\n",
        "#         # Update progress bar\n",
        "#         progress_bar.progress((i + 1) / total_rows)\n",
        "\n",
        "#     # Convert results to DataFrame and join with original df\n",
        "#     scores_df = pd.DataFrame(results)\n",
        "#     df[['positive', 'negative', 'neutral', 'compound']] = scores_df[['pos', 'neg', 'neu', 'compound']]\n",
        "\n",
        "#     # Categorize sentiment\n",
        "#     df['Sentiment'] = df.apply(lambda row: \"Positive\" if row['compound'] > 0.05\n",
        "#                               else \"Negative\" if row['compound'] < -0.05 else \"Neutral\", axis=1)\n",
        "\n",
        "#     # Remove progress bar when done\n",
        "#     progress_bar.empty()\n",
        "\n",
        "#     return df\n",
        "\n",
        "# def plot_sentiment_charts(df):\n",
        "#     \"\"\"Plot sentiment distribution as both a pie chart and a bar chart.\"\"\"\n",
        "#     sentiment_counts = df['Sentiment'].value_counts()\n",
        "\n",
        "#     labels = sentiment_counts.index.tolist()\n",
        "#     sizes = sentiment_counts.values.tolist()\n",
        "#     colors = {'positive': '#3bed68', 'negative': '#ed184d', 'neutral': '#1776eb'}\n",
        "\n",
        "#     # Convert labels to lowercase to match the keys in the colors dictionary\n",
        "#     color_list = [colors[label.lower()] for label in labels]\n",
        "\n",
        "#     # Create layout for two charts\n",
        "#     col1, col2 = st.columns(2)\n",
        "\n",
        "#     # Pie Chart\n",
        "#     with col1:\n",
        "#         plt.figure(figsize=(5, 5))\n",
        "#         plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=color_list, startangle=140)\n",
        "#         plt.title('Sentiment Distribution')\n",
        "#         st.pyplot(plt)\n",
        "\n",
        "#     # Bar Chart\n",
        "#     with col2:\n",
        "#         plt.figure(figsize=(4.5, 5))\n",
        "#         sns.barplot(x=labels, y=sizes, palette=color_list)\n",
        "#         plt.ylabel(\"Count\")\n",
        "#         plt.title(\"Sentiment Distribution\")\n",
        "#         st.pyplot(plt)\n",
        "\n",
        "#     # Summary Text\n",
        "#     dominant_sentiment = sentiment_counts.idxmax()\n",
        "#     st.write(f\"### Summary: The overall sentiment of the data is **{dominant_sentiment}**.\")\n",
        "\n",
        "# # Streamlit UI\n",
        "# st.title(\"Advanced Sentiment Analysis with RoBERTa\")\n",
        "# st.markdown(\"\"\"\n",
        "# This app uses the **cardiffnlp/twitter-roberta-base-sentiment** model,\n",
        "# which performs better on longer texts than VADER.\n",
        "# \"\"\")\n",
        "\n",
        "# # Tabs for different functionalities\n",
        "# tabs = st.tabs([\"Upload CSV\", \"Text Input\"])\n",
        "\n",
        "# # Part 1: File Upload Sentiment Analysis\n",
        "# with tabs[0]:\n",
        "#     st.header(\"Upload CSV for Sentiment Analysis\")\n",
        "#     uploaded_file = st.file_uploader(\"Upload a CSV file with a 'Feedback', 'Review', or 'Text' column\", type=[\"csv\"])\n",
        "\n",
        "#     if uploaded_file:\n",
        "#         with st.spinner(\"Analyzing sentiment... This may take a moment for large files.\"):\n",
        "#             df_result = process_uploaded_file(uploaded_file)\n",
        "\n",
        "#         if df_result is not None:\n",
        "#             st.write(\"### Sentiment Analysis Results\")\n",
        "\n",
        "#             # Display tables for each sentiment category\n",
        "#             for sentiment in ['Positive', 'Negative', 'Neutral']:\n",
        "#                 # Determine which column to display\n",
        "#                 display_column = next((col for col in ['Review', 'Feedback', 'Text'] if col in df_result.columns), None)\n",
        "\n",
        "#                 if display_column:\n",
        "#                     filtered_df = df_result[df_result['Sentiment'] == sentiment][[display_column, 'Sentiment', 'positive', 'negative', 'neutral']]\n",
        "#                     if not filtered_df.empty:\n",
        "#                         st.write(f\"### {sentiment} Reviews ({len(filtered_df)} total)\")\n",
        "#                         st.dataframe(filtered_df.reset_index(drop=True))\n",
        "\n",
        "#             plot_sentiment_charts(df_result)\n",
        "\n",
        "#             # Option to download the analyzed data\n",
        "#             csv = df_result.to_csv(index=False)\n",
        "#             st.download_button(\n",
        "#                 label=\"Download analyzed data as CSV\",\n",
        "#                 data=csv,\n",
        "#                 file_name=\"sentiment_analysis_results.csv\",\n",
        "#                 mime=\"text/csv\",\n",
        "#             )\n",
        "\n",
        "# # Part 2: Text Input Sentiment Analysis\n",
        "# with tabs[1]:\n",
        "#     st.header(\"Type Your Text for Sentiment Analysis\")\n",
        "#     user_text = st.text_area(\"Enter text here (works well with longer reviews):\", height=200)\n",
        "\n",
        "#     if st.button(\"Calculate\", help=\"Click to analyze sentiment\"):\n",
        "#         if user_text.strip():  # Ensure input is not empty\n",
        "#             with st.spinner(\"Analyzing sentiment...\"):\n",
        "#                 result = analyze_sentiment(user_text)\n",
        "\n",
        "#             st.write(\"### Sentiment Scores\")\n",
        "\n",
        "#             def sentiment_bar(label, value, color):\n",
        "#                 \"\"\"Function to create a sentiment bar visualization\"\"\"\n",
        "#                 bar_html = f\"\"\"\n",
        "#                 <div style=\"margin-bottom: 1rem\">\n",
        "#                     <div style=\"width: {value*100}%; height: 10px; background-color: {color}; padding: 2px; border-radius: 5px; text-align: center; color: white;\">\n",
        "#                     </div>\n",
        "#                     {label}: {value:.3f}\n",
        "#                 </div>\n",
        "#                 \"\"\"\n",
        "#                 st.markdown(bar_html, unsafe_allow_html=True)\n",
        "\n",
        "#             # Displaying sentiment bars\n",
        "#             sentiment_bar(\"Negative\", result['neg'], \"#ed187b\")\n",
        "#             sentiment_bar(\"Neutral\", result['neu'], \"#1776eb\")\n",
        "#             sentiment_bar(\"Positive\", result['pos'], \"#3bed68\")\n",
        "\n",
        "#             # Displaying compound score\n",
        "#             compound_score = result['compound']\n",
        "#             sentiment_label = \"Positive\" if compound_score > 0.05 else \"Negative\" if compound_score < -0.05 else \"Neutral\"\n",
        "#             st.write(f\"Compound Score: {compound_score:.4f}\")\n",
        "#             st.write(f\"### Overall Sentiment: **{sentiment_label}**\")\n",
        "#         else:\n",
        "#             st.warning(\"Please enter some text before calculating.\")\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import pipeline, RobertaTokenizer\n",
        "import torch\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Check if CUDA is available and set device accordingly\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# 1. Optimized Model Initialization with proper caching\n",
        "@st.cache_resource\n",
        "def load_sentiment_model():\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "    model = pipeline(\n",
        "        \"sentiment-analysis\",\n",
        "        model=\"cardiffnlp/twitter-roberta-base-sentiment\",\n",
        "        return_all_scores=True,\n",
        "        device=device\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "# Load model and tokenizer once\n",
        "sentiment_pipeline, tokenizer = load_sentiment_model()\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"Analyze sentiment using RoBERTa.\"\"\"\n",
        "    max_length = 512  # RoBERTa's max token length\n",
        "    # Tokenize the text and check its length\n",
        "    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    # If the token length exceeds the limit, chunk the text\n",
        "    if len(tokens['input_ids'][0]) > max_length:\n",
        "        # Split the text into chunks based on token length\n",
        "        chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
        "        results = []\n",
        "        for chunk in chunks:\n",
        "            results.append(sentiment_pipeline(chunk)[0])\n",
        "\n",
        "        # Average the scores across chunks\n",
        "        neg = sum(r[0]['score'] for r in results) / len(results)\n",
        "        neu = sum(r[1]['score'] for r in results) / len(results)\n",
        "        pos = sum(r[2]['score'] for r in results) / len(results)\n",
        "    else:\n",
        "        # Process the text if it fits within the token limit\n",
        "        result = sentiment_pipeline(text)[0]\n",
        "        neg = result[0]['score']\n",
        "        neu = result[1]['score']\n",
        "        pos = result[2]['score']\n",
        "\n",
        "    compound = pos - neg\n",
        "\n",
        "    return {\n",
        "        'pos': pos,\n",
        "        'neg': neg,\n",
        "        'neu': neu,\n",
        "        'compound': compound\n",
        "    }\n",
        "\n",
        "# 2. Batch processing implementation\n",
        "def process_uploaded_file(uploaded_file):\n",
        "    \"\"\"Process uploaded CSV file and analyze sentiment using parallel processing.\"\"\"\n",
        "    with st.spinner(\"Loading file...\"):\n",
        "        df = pd.read_csv(uploaded_file)\n",
        "\n",
        "    # Determine which column to use for sentiment analysis\n",
        "    valid_columns = ['Feedback', 'Review', 'Text']\n",
        "    selected_column = next((col for col in valid_columns if col in df.columns), None)\n",
        "\n",
        "    if not selected_column:\n",
        "        st.error(\"CSV must contain a 'Feedback', 'Review', or 'Text' column.\")\n",
        "        return None\n",
        "\n",
        "    total_rows = len(df)\n",
        "    st.info(f\"File loaded: {total_rows} rows\")\n",
        "\n",
        "    # Create a progress bar\n",
        "    progress_bar = st.progress(0)\n",
        "    status_text = st.empty()\n",
        "\n",
        "    # Prepare texts as strings\n",
        "    texts = [str(text) for text in df[selected_column]]\n",
        "\n",
        "    # Determine optimal approach based on size\n",
        "    if total_rows > 500:  # For larger datasets\n",
        "        # Use parallel processing with ThreadPoolExecutor\n",
        "        results = []\n",
        "        batch_size = 20  # Process in batches for better progress tracking\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your system\n",
        "            for i in range(0, total_rows, batch_size):\n",
        "                end_idx = min(i + batch_size, total_rows)\n",
        "                batch_texts = texts[i:end_idx]\n",
        "\n",
        "                # Process batch in parallel\n",
        "                batch_results = list(executor.map(analyze_sentiment, batch_texts))\n",
        "                results.extend(batch_results)\n",
        "\n",
        "                # Update progress\n",
        "                progress = min(1.0, end_idx / total_rows)\n",
        "                progress_bar.progress(progress)\n",
        "                status_text.text(f\"Processing rows {i+1}-{end_idx} of {total_rows}\")\n",
        "    else:\n",
        "        # For smaller datasets, simpler processing is fine\n",
        "        results = []\n",
        "        for i, text in enumerate(texts):\n",
        "            results.append(analyze_sentiment(text))\n",
        "            # Update progress\n",
        "            progress_bar.progress((i + 1) / total_rows)\n",
        "            status_text.text(f\"Processing row {i+1}/{total_rows}\")\n",
        "\n",
        "    # Convert results to DataFrame and join with original df\n",
        "    scores_df = pd.DataFrame(results)\n",
        "    df[['positive', 'negative', 'neutral', 'compound']] = scores_df[['pos', 'neg', 'neu', 'compound']]\n",
        "\n",
        "    # Categorize sentiment\n",
        "    df['Sentiment'] = df.apply(lambda row: \"Positive\" if row['compound'] > 0.05\n",
        "                              else \"Negative\" if row['compound'] < -0.05 else \"Neutral\", axis=1)\n",
        "\n",
        "    # Remove progress elements when done\n",
        "    progress_bar.empty()\n",
        "    status_text.empty()\n",
        "\n",
        "    return df\n",
        "\n",
        "def plot_sentiment_charts(df):\n",
        "    \"\"\"Plot sentiment distribution as both a pie chart and a bar chart.\"\"\"\n",
        "    sentiment_counts = df['Sentiment'].value_counts()\n",
        "\n",
        "    labels = sentiment_counts.index.tolist()\n",
        "    sizes = sentiment_counts.values.tolist()\n",
        "    colors = {'positive': '#3bed68', 'negative': '#ed184d', 'neutral': '#1776eb'}\n",
        "\n",
        "    # Convert labels to lowercase to match the keys in the colors dictionary\n",
        "    color_list = [colors[label.lower()] for label in labels]\n",
        "\n",
        "    # Create layout for two charts\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    # Pie Chart\n",
        "    with col1:\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=color_list, startangle=140)\n",
        "        plt.title('Sentiment Distribution')\n",
        "        st.pyplot(plt)\n",
        "\n",
        "    # Bar Chart\n",
        "    with col2:\n",
        "        plt.figure(figsize=(4.5, 5))\n",
        "        sns.barplot(x=labels, y=sizes, palette=color_list)\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.title(\"Sentiment Distribution\")\n",
        "        st.pyplot(plt)\n",
        "\n",
        "    # Summary Text\n",
        "    dominant_sentiment = sentiment_counts.idxmax()\n",
        "    st.write(f\"### Summary: The overall sentiment of the data is **{dominant_sentiment}**.\")\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Advanced Sentiment Analysis with RoBERTa\")\n",
        "st.markdown(\"\"\"\n",
        "This app uses the **cardiffnlp/twitter-roberta-base-sentiment** model,\n",
        "which performs better on longer texts than VADER.\n",
        "\"\"\")\n",
        "\n",
        "# Tabs for different functionalities\n",
        "tabs = st.tabs([\"Upload CSV\", \"Text Input\"])\n",
        "\n",
        "# Part 1: File Upload Sentiment Analysis\n",
        "with tabs[0]:\n",
        "    st.header(\"Upload CSV for Sentiment Analysis\")\n",
        "    uploaded_file = st.file_uploader(\"Upload a CSV file with a 'Feedback', 'Review', or 'Text' column\", type=[\"csv\"])\n",
        "\n",
        "    if uploaded_file:\n",
        "        with st.spinner(\"Analyzing sentiment... This may take a moment for large files.\"):\n",
        "            df_result = process_uploaded_file(uploaded_file)\n",
        "\n",
        "        if df_result is not None:\n",
        "            st.write(\"### Sentiment Analysis Results\")\n",
        "\n",
        "            # Display tables for each sentiment category\n",
        "            for sentiment in ['Positive', 'Negative', 'Neutral']:\n",
        "                # Determine which column to display\n",
        "                display_column = next((col for col in ['Review', 'Feedback', 'Text'] if col in df_result.columns), None)\n",
        "\n",
        "                if display_column:\n",
        "                    filtered_df = df_result[df_result['Sentiment'] == sentiment][[display_column, 'Sentiment', 'positive', 'negative', 'neutral']]\n",
        "                    if not filtered_df.empty:\n",
        "                        st.write(f\"### {sentiment} Reviews ({len(filtered_df)} total)\")\n",
        "                        st.dataframe(filtered_df.reset_index(drop=True))\n",
        "\n",
        "            plot_sentiment_charts(df_result)\n",
        "\n",
        "            # Option to download the analyzed data\n",
        "            csv = df_result.to_csv(index=False)\n",
        "            st.download_button(\n",
        "                label=\"Download analyzed data as CSV\",\n",
        "                data=csv,\n",
        "                file_name=\"sentiment_analysis_results.csv\",\n",
        "                mime=\"text/csv\",\n",
        "            )\n",
        "\n",
        "# Part 2: Text Input Sentiment Analysis\n",
        "with tabs[1]:\n",
        "    st.header(\"Type Your Text for Sentiment Analysis\")\n",
        "    user_text = st.text_area(\"Enter text here (works well with longer reviews):\", height=200)\n",
        "\n",
        "    if st.button(\"Calculate\", help=\"Click to analyze sentiment\"):\n",
        "        if user_text.strip():  # Ensure input is not empty\n",
        "            with st.spinner(\"Analyzing sentiment...\"):\n",
        "                result = analyze_sentiment(user_text)\n",
        "\n",
        "            st.write(\"### Sentiment Scores\")\n",
        "\n",
        "            def sentiment_bar(label, value, color):\n",
        "                \"\"\"Function to create a sentiment bar visualization\"\"\"\n",
        "                bar_html = f\"\"\"\n",
        "                <div style=\"margin-bottom: 1rem\">\n",
        "                    <div style=\"width: {value*100}%; height: 10px; background-color: {color}; padding: 2px; border-radius: 5px; text-align: center; color: white;\">\n",
        "                    </div>\n",
        "                    {label}: {value:.3f}\n",
        "                </div>\n",
        "                \"\"\"\n",
        "                st.markdown(bar_html, unsafe_allow_html=True)\n",
        "\n",
        "            # Displaying sentiment bars\n",
        "            sentiment_bar(\"Negative\", result['neg'], \"#ed187b\")\n",
        "            sentiment_bar(\"Neutral\", result['neu'], \"#1776eb\")\n",
        "            sentiment_bar(\"Positive\", result['pos'], \"#3bed68\")\n",
        "\n",
        "            # Displaying compound score\n",
        "            compound_score = result['compound']\n",
        "            sentiment_label = \"Positive\" if compound_score > 0.05 else \"Negative\" if compound_score < -0.05 else \"Neutral\"\n",
        "            st.write(f\"Compound Score: {compound_score:.4f}\")\n",
        "            st.write(f\"### Overall Sentiment: **{sentiment_label}**\")\n",
        "        else:\n",
        "            st.warning(\"Please enter some text before calculating.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viF-uR5dS2RB",
        "outputId": "45c56072-5803-4083-b83a-e778e4240a98"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok tunnels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t93f70ejhK4w",
        "outputId": "edaca1a4-732b-49ee-a9c1-185b1ce96050"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok - tunnel local ports to public URLs and inspect traffic\n",
            "\n",
            "USAGE:\n",
            "  ngrok [command] [flags]\n",
            "\n",
            "COMMANDS: \n",
            "  config          update or migrate ngrok's configuration file\n",
            "  http            start an HTTP tunnel\n",
            "  tcp             start a TCP tunnel\n",
            "  tunnel          start a tunnel for use with a tunnel-group backend\n",
            "\n",
            "EXAMPLES: \n",
            "  ngrok http 80                                                 # secure public URL for port 80 web server\n",
            "  ngrok http --url baz.ngrok.dev 8080                           # port 8080 available at baz.ngrok.dev\n",
            "  ngrok tcp 22                                                  # tunnel arbitrary TCP traffic to port 22\n",
            "  ngrok http 80 --oauth=google --oauth-allow-email=foo@foo.com  # secure your app with oauth\n",
            "\n",
            "Paid Features: \n",
            "  ngrok http 80 --url mydomain.com                              # run ngrok with your own custom domain\n",
            "  ngrok http 80 --cidr-allow 2600:8c00::a03c:91ee:fe69:9695/32  # run ngrok with IP policy restrictions\n",
            "  Upgrade your account at https://dashboard.ngrok.com/billing/subscription to access paid features\n",
            "\n",
            "Upgrade your account at https://dashboard.ngrok.com/billing/subscription to access paid features\n",
            "\n",
            "Flags:\n",
            "  -h, --help      help for ngrok\n",
            "\n",
            "Use \"ngrok [command] --help\" for more information about a command.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zUPLbY6yjrzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f ngrok\n"
      ],
      "metadata": {
        "id": "s8ZUcSSxhT-g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Run Streamlit in the background\n",
        "os.system(\"streamlit run app.py &\")\n",
        "\n",
        "# Wait for the server to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Create Ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Streamlit app is live at:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwWA8684TJxj",
        "outputId": "f68e75e4-d248-4e19-b78c-27f617b519ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is live at: NgrokTunnel: \"https://f8d9-34-16-205-121.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CN2EgG29TY9_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}